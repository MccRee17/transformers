06/30 02:00:31 AM The args: Namespace(act='gelu', aug_train=False, cache_dir='', data_dir='./glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, learning_rate=5e-05, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, output_dir='tmp_ours_2quad', pred_distill=False, seed=42, softmax_act='softmax', student_model='./TinyBERT_General_4L_312D_2quad', task_name='MNLI', teacher_model='./checkpoint-36500', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
06/30 02:00:31 AM device: cuda n_gpu: 1
06/30 02:00:39 AM Writing example 0 of 392702
06/30 02:00:39 AM *** Example ***
06/30 02:00:39 AM guid: train-0
06/30 02:00:39 AM tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
06/30 02:00:39 AM input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/30 02:00:39 AM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/30 02:00:39 AM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/30 02:00:39 AM label: neutral
06/30 02:00:39 AM label_id: 1
06/30 02:00:44 AM Writing example 10000 of 392702
06/30 02:00:50 AM Writing example 20000 of 392702
06/30 02:00:56 AM Writing example 30000 of 392702
06/30 02:01:02 AM Writing example 40000 of 392702
06/30 02:01:08 AM Writing example 50000 of 392702
06/30 02:01:14 AM Writing example 60000 of 392702
06/30 02:01:19 AM Writing example 70000 of 392702
06/30 02:01:25 AM Writing example 80000 of 392702
06/30 02:01:31 AM Writing example 90000 of 392702
06/30 02:01:37 AM Writing example 100000 of 392702
06/30 02:01:43 AM Writing example 110000 of 392702
06/30 02:01:49 AM Writing example 120000 of 392702
06/30 02:01:54 AM Writing example 130000 of 392702
06/30 02:02:01 AM Writing example 140000 of 392702
06/30 02:02:07 AM Writing example 150000 of 392702
06/30 02:02:13 AM Writing example 160000 of 392702
06/30 02:02:18 AM Writing example 170000 of 392702
06/30 02:02:24 AM Writing example 180000 of 392702
06/30 02:02:30 AM Writing example 190000 of 392702
06/30 02:02:37 AM Writing example 200000 of 392702
06/30 02:02:42 AM Writing example 210000 of 392702
06/30 02:02:48 AM Writing example 220000 of 392702
06/30 02:02:54 AM Writing example 230000 of 392702
06/30 02:03:00 AM Writing example 240000 of 392702
06/30 02:03:05 AM Writing example 250000 of 392702
06/30 02:03:11 AM Writing example 260000 of 392702
06/30 02:03:17 AM Writing example 270000 of 392702
06/30 02:03:24 AM Writing example 280000 of 392702
06/30 02:03:30 AM Writing example 290000 of 392702
06/30 02:03:36 AM Writing example 300000 of 392702
06/30 02:03:41 AM Writing example 310000 of 392702
06/30 02:03:47 AM Writing example 320000 of 392702
06/30 02:03:53 AM Writing example 330000 of 392702
06/30 02:03:59 AM Writing example 340000 of 392702
06/30 02:04:05 AM Writing example 350000 of 392702
06/30 02:04:10 AM Writing example 360000 of 392702
06/30 02:04:18 AM Writing example 370000 of 392702
06/30 02:04:24 AM Writing example 380000 of 392702
06/30 02:04:30 AM Writing example 390000 of 392702
06/30 02:04:35 AM Writing example 0 of 9815
06/30 02:04:35 AM *** Example ***
06/30 02:04:35 AM guid: dev_matched-0
06/30 02:04:35 AM tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
06/30 02:04:35 AM input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/30 02:04:35 AM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/30 02:04:35 AM segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/30 02:04:35 AM label: neutral
06/30 02:04:35 AM label_id: 1
06/30 02:04:41 AM Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "neutral",
    "2": "contradiction"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "contradiction": 2,
    "entailment": 0,
    "neutral": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

using softmax_act: <function softmax at 0x7fe5e2806f28>
using act: <function gelu at 0x7fe5abdeb9d8>
using softmax_act: <function softmax at 0x7fe5e2806f28>
using act: <function gelu at 0x7fe5abdeb9d8>
using softmax_act: <function softmax at 0x7fe5e2806f28>
using act: <function gelu at 0x7fe5abdeb9d8>
using softmax_act: <function softmax at 0x7fe5e2806f28>
using act: <function gelu at 0x7fe5abdeb9d8>
using softmax_act: <function softmax at 0x7fe5e2806f28>
using act: <function gelu at 0x7fe5abdeb9d8>
using softmax_act: <function softmax at 0x7fe5e2806f28>
using act: <function gelu at 0x7fe5abdeb9d8>
using softmax_act: <function softmax at 0x7fe5e2806f28>
using act: <function gelu at 0x7fe5abdeb9d8>
using softmax_act: <function softmax at 0x7fe5e2806f28>
using act: <function gelu at 0x7fe5abdeb9d8>
using softmax_act: <function softmax at 0x7fe5e2806f28>
using act: <function gelu at 0x7fe5abdeb9d8>
using softmax_act: <function softmax at 0x7fe5e2806f28>
using act: <function gelu at 0x7fe5abdeb9d8>
using softmax_act: <function softmax at 0x7fe5e2806f28>
using act: <function gelu at 0x7fe5abdeb9d8>
using softmax_act: <function softmax at 0x7fe5e2806f28>
using act: <function gelu at 0x7fe5abdeb9d8>
06/30 02:04:43 AM Loading model ./checkpoint-36500/pytorch_model.bin
./checkpoint-36500/pytorch_model.bin
06/30 02:04:44 AM loading model...
06/30 02:04:44 AM done!
06/30 02:04:44 AM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
06/30 02:04:44 AM Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
06/30 02:05:15 AM ***** Teacher evaluation *****
06/30 02:05:15 AM {'acc': 0.8398369842078451, 'eval_loss': 0.4336751479085183}
06/30 02:05:15 AM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "softmax_act": "2quad",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

using softmax_act: <function softmax_2quad at 0x7fe5abdebbf8>
using act: <function quad at 0x7fe5abdebae8>
using softmax_act: <function softmax_2quad at 0x7fe5abdebbf8>
using act: <function quad at 0x7fe5abdebae8>
using softmax_act: <function softmax_2quad at 0x7fe5abdebbf8>
using act: <function quad at 0x7fe5abdebae8>
using softmax_act: <function softmax_2quad at 0x7fe5abdebbf8>
using act: <function quad at 0x7fe5abdebae8>
06/30 02:05:15 AM Loading model ./TinyBERT_General_4L_312D_2quad/pytorch_model.bin
./TinyBERT_General_4L_312D_2quad/pytorch_model.bin
06/30 02:05:15 AM loading model...
06/30 02:05:15 AM done!
06/30 02:05:15 AM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
06/30 02:05:15 AM Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
06/30 02:05:15 AM ***** Running training *****
06/30 02:05:15 AM   Num examples = 392702
06/30 02:05:15 AM   Batch size = 32
06/30 02:05:15 AM   Num steps = 122710
06/30 02:05:15 AM n: bert.embeddings.word_embeddings.weight
06/30 02:05:15 AM n: bert.embeddings.position_embeddings.weight
06/30 02:05:15 AM n: bert.embeddings.token_type_embeddings.weight
06/30 02:05:15 AM n: bert.embeddings.LayerNorm.weight
06/30 02:05:15 AM n: bert.embeddings.LayerNorm.bias
06/30 02:05:15 AM n: bert.encoder.layer.0.attention.self.query.weight
06/30 02:05:15 AM n: bert.encoder.layer.0.attention.self.query.bias
06/30 02:05:15 AM n: bert.encoder.layer.0.attention.self.key.weight
06/30 02:05:15 AM n: bert.encoder.layer.0.attention.self.key.bias
06/30 02:05:15 AM n: bert.encoder.layer.0.attention.self.value.weight
06/30 02:05:15 AM n: bert.encoder.layer.0.attention.self.value.bias
06/30 02:05:15 AM n: bert.encoder.layer.0.attention.output.dense.weight
06/30 02:05:15 AM n: bert.encoder.layer.0.attention.output.dense.bias
06/30 02:05:15 AM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
06/30 02:05:15 AM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
06/30 02:05:15 AM n: bert.encoder.layer.0.intermediate.dense.weight
06/30 02:05:15 AM n: bert.encoder.layer.0.intermediate.dense.bias
06/30 02:05:15 AM n: bert.encoder.layer.0.output.dense.weight
06/30 02:05:15 AM n: bert.encoder.layer.0.output.dense.bias
06/30 02:05:15 AM n: bert.encoder.layer.0.output.LayerNorm.weight
06/30 02:05:15 AM n: bert.encoder.layer.0.output.LayerNorm.bias
06/30 02:05:15 AM n: bert.encoder.layer.1.attention.self.query.weight
06/30 02:05:15 AM n: bert.encoder.layer.1.attention.self.query.bias
06/30 02:05:15 AM n: bert.encoder.layer.1.attention.self.key.weight
06/30 02:05:15 AM n: bert.encoder.layer.1.attention.self.key.bias
06/30 02:05:15 AM n: bert.encoder.layer.1.attention.self.value.weight
06/30 02:05:15 AM n: bert.encoder.layer.1.attention.self.value.bias
06/30 02:05:15 AM n: bert.encoder.layer.1.attention.output.dense.weight
06/30 02:05:15 AM n: bert.encoder.layer.1.attention.output.dense.bias
06/30 02:05:15 AM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
06/30 02:05:15 AM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
06/30 02:05:15 AM n: bert.encoder.layer.1.intermediate.dense.weight
06/30 02:05:15 AM n: bert.encoder.layer.1.intermediate.dense.bias
06/30 02:05:15 AM n: bert.encoder.layer.1.output.dense.weight
06/30 02:05:15 AM n: bert.encoder.layer.1.output.dense.bias
06/30 02:05:15 AM n: bert.encoder.layer.1.output.LayerNorm.weight
06/30 02:05:15 AM n: bert.encoder.layer.1.output.LayerNorm.bias
06/30 02:05:15 AM n: bert.encoder.layer.2.attention.self.query.weight
06/30 02:05:15 AM n: bert.encoder.layer.2.attention.self.query.bias
06/30 02:05:15 AM n: bert.encoder.layer.2.attention.self.key.weight
06/30 02:05:15 AM n: bert.encoder.layer.2.attention.self.key.bias
06/30 02:05:15 AM n: bert.encoder.layer.2.attention.self.value.weight
06/30 02:05:15 AM n: bert.encoder.layer.2.attention.self.value.bias
06/30 02:05:15 AM n: bert.encoder.layer.2.attention.output.dense.weight
06/30 02:05:15 AM n: bert.encoder.layer.2.attention.output.dense.bias
06/30 02:05:15 AM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
06/30 02:05:15 AM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
06/30 02:05:15 AM n: bert.encoder.layer.2.intermediate.dense.weight
06/30 02:05:15 AM n: bert.encoder.layer.2.intermediate.dense.bias
06/30 02:05:15 AM n: bert.encoder.layer.2.output.dense.weight
06/30 02:05:15 AM n: bert.encoder.layer.2.output.dense.bias
06/30 02:05:15 AM n: bert.encoder.layer.2.output.LayerNorm.weight
06/30 02:05:15 AM n: bert.encoder.layer.2.output.LayerNorm.bias
06/30 02:05:15 AM n: bert.encoder.layer.3.attention.self.query.weight
06/30 02:05:15 AM n: bert.encoder.layer.3.attention.self.query.bias
06/30 02:05:15 AM n: bert.encoder.layer.3.attention.self.key.weight
06/30 02:05:15 AM n: bert.encoder.layer.3.attention.self.key.bias
06/30 02:05:15 AM n: bert.encoder.layer.3.attention.self.value.weight
06/30 02:05:15 AM n: bert.encoder.layer.3.attention.self.value.bias
06/30 02:05:15 AM n: bert.encoder.layer.3.attention.output.dense.weight
06/30 02:05:15 AM n: bert.encoder.layer.3.attention.output.dense.bias
06/30 02:05:15 AM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
06/30 02:05:15 AM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
06/30 02:05:15 AM n: bert.encoder.layer.3.intermediate.dense.weight
06/30 02:05:15 AM n: bert.encoder.layer.3.intermediate.dense.bias
06/30 02:05:15 AM n: bert.encoder.layer.3.output.dense.weight
06/30 02:05:15 AM n: bert.encoder.layer.3.output.dense.bias
06/30 02:05:15 AM n: bert.encoder.layer.3.output.LayerNorm.weight
06/30 02:05:15 AM n: bert.encoder.layer.3.output.LayerNorm.bias
06/30 02:05:15 AM n: bert.pooler.dense.weight
06/30 02:05:15 AM n: bert.pooler.dense.bias
06/30 02:05:15 AM n: classifier.weight
06/30 02:05:15 AM n: classifier.bias
06/30 02:05:15 AM n: fit_dense.weight
06/30 02:05:15 AM n: fit_dense.bias
06/30 02:05:15 AM Total parameters: 14591571
06/30 02:05:45 AM ***** Running evaluation *****
06/30 02:05:45 AM   Epoch = 0 iter 199 step
06/30 02:05:45 AM   Num examples = 9815
06/30 02:05:45 AM   Batch size = 32
06/30 02:05:45 AM ***** Eval results *****
06/30 02:05:45 AM   att_loss = 6.8030063782505055
06/30 02:05:45 AM   cls_loss = 0.0
06/30 02:05:45 AM   global_step = 199
06/30 02:05:45 AM   loss = 8.343805552727014
06/30 02:05:45 AM   rep_loss = 1.540799170283217
06/30 02:05:45 AM ***** Save model *****
